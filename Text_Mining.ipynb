{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1hDk9hDvOH7",
    "outputId": "e8bfdd43-279b-4ba6-86d8-3704f259018b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "#Change current working directory to gdrive\n",
    "%cd /gdrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_k0z66e38lG",
    "outputId": "b031027f-28ac-4edc-fa6d-84fe28ea88cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.19.5)\n",
      "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.0.1)\n",
      "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.4.0)\n",
      "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (3.0.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.5.1->mlxtend) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "#NLTK-------------------------------\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stemporter import PorterStemmer\n",
    "\n",
    "# Import libraries for feature \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "!pip install mlxtend\n",
    "import joblib\n",
    "import sys\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMwGQK7KAd7T",
    "outputId": "4ee4466e-8e55-401e-ae6d-87beb9e7be9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 2)\n",
      "(2070, 17)\n"
     ]
    }
   ],
   "source": [
    "#Read files\n",
    "textfile = r'/gdrive/My Drive/CIS508/Assignment_5/Comments.csv'\n",
    "textData = pd.read_csv(textfile) #creates a dataframe\n",
    "\n",
    "CustInfofile = r'/gdrive/My Drive/CIS508/Assignment_5/Customers.csv'\n",
    "CustInfoData = pd.read_csv(CustInfofile)  #creates a dataframe\n",
    "\n",
    "print(textData.shape)\n",
    "print(CustInfoData.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWOTk6C1Ao45",
    "outputId": "4126f871-4c09-42ce-9670-0f2d41dcaa11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 16)\n",
      "(2070, 2)\n",
      "     ID                                           Comments\n",
      "0  1309  Does not like the way the phone works. It is t...\n",
      "1  3556  Wanted to know the nearest store location. Wan...\n",
      "2  2230  Wants to know how to do text messaging. Referr...\n",
      "3  2312  Asked how to disable call waiting. referred hi...\n",
      "4  3327  Needs help learning how to use the phone. I su...\n",
      "0       Cancelled\n",
      "1         Current\n",
      "2         Current\n",
      "3         Current\n",
      "4       Cancelled\n",
      "          ...    \n",
      "2065    Cancelled\n",
      "2066    Cancelled\n",
      "2067    Cancelled\n",
      "2068    Cancelled\n",
      "2069    Cancelled\n",
      "Name: TARGET, Length: 2070, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Extract target column from Customer Info file\n",
    "y_train = CustInfoData[\"TARGET\"]\n",
    "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
    "                     \n",
    "print(X_train.shape)\n",
    "print(textData.shape)\n",
    "print(textData.head())\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuWYNz2Ep17l"
   },
   "outputs": [],
   "source": [
    "#Tokenize - Split the sentences to lists of words\n",
    "textData['CommentsTokenized'] = textData['Comments'].apply(word_tokenize)\n",
    "\n",
    "export_csv = textData.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/TextDataTokenized1.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeYXLU-u_v9R"
   },
   "outputs": [],
   "source": [
    "# Use Snowball stemmer.\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#Now do stemming - create a new dataframe to store stemmed version\n",
    "newTextData=pd.DataFrame()\n",
    "newTextData=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
    "newTextData['CommentsTokenizedStemmed'] = textData['CommentsTokenized'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "\n",
    "export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/SnownewTextDataTS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSJqDLlTACTG"
   },
   "outputs": [],
   "source": [
    "# Use Porter stemmer.\n",
    "stemmer_1 = PorterStemmer()\n",
    "\n",
    "#Now do stemming - create a new dataframe to store stemmed version\n",
    "newTextData_1=pd.DataFrame()\n",
    "newTextData_1=textData.drop(columns=[\"CommentsTokenized\",\"Comments\"])\n",
    "newTextData_1['CommentsTokenizedStemmed_1'] = textData['CommentsTokenized'].apply(lambda x: [stemmer_1.stem(y) for y in x]) # Stem every word.\n",
    "\n",
    "export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/Porter_newTextDataTS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M07E7VW7_y0d"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Join stemmed strings\n",
    "newTextData['CommentsTokenizedStemmed'] = newTextData['CommentsTokenizedStemmed'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "export_csv = newTextData.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/newTextData-Joined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WiBguQloljam",
    "outputId": "d4fab83a-d732-49ff-dbca-9a76c5ac38b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 354)\n",
      "int64\n",
      "['3399', '3g', 'abysm', 'access', 'accessori', 'adapt', 'add', 'addit', 'additon', 'address', 'adit', 'adress', 'advertis', 'afraid', 'alway', 'angel', 'angri', 'ani', 'anoth', 'anyth', 'anytim', 'area', 'asap', 'ask', 'bad', 'basic', 'bateri', 'batteri', 'becaus', 'believ', 'better', 'bigger', 'book', 'bought', 'brain', 'bring', 'built', 'busi', 'button', 'buy', 'cancel', 'cancer', 'car', 'care', 'carrier', 'caus', 'cc', 'cell', 'certain', 'chang', 'charg', 'charger', 'check', 'chip', 'citi', 'claim', 'cleariti', 'cold', 'comapr', 'compani', 'compar', 'competit', 'complain', 'complaint', 'concept', 'connect', 'consisit', 'consist', 'constan', 'contact', 'continu', 'contract', 'correct', 'cost', 'coupl', 'cover', 'coverag', 'creat', 'credit', 'cstmer', 'cstmr', 'current', 'cust', 'custom', 'customr', 'date', 'day', 'dead', 'decent', 'defect', 'deo', 'did', 'die', 'differ', 'difficult', 'digiti', 'direct', 'disabl', 'doe', 'don', 'dont', 'drop', 'dure', 'easier', 'effect', 'encount', 'end', 'enemi', 'equip', 'everytim', 'everywher', 'evrey', 'exact', 'expect', 'expir', 'explain', 'facepl', 'fals', 'famili', 'featur', 'fed', 'figur', 'fine', 'fix', 'forev', 'forward', 'friend', 'function', 'furthermor', 'futur', 'gave', 'goat', 'good', 'great', 'gsm', 'handset', 'happi', 'hard', 'hate', 'hear', 'heard', 'help', 'higher', 'highway', 'hochi', 'hole', 'home', 'hope', 'horribl', 'hous', 'implement', 'improv', 'inadequ', 'includ', 'info', 'inform', 'ing', 'internet', 'intersect', 'issu', 'june', 'just', 'kid', 'kno', 'know', 'lame', 'later', 'lctn', 'learn', 'leroy', 'like', 'line', 'list', 'local', 'locat', 'locatn', 'long', 'los', 'lost', 'lot', 'love', 'major', 'make', 'manag', 'mani', 'manual', 'market', 'mean', 'messag', 'metropolitian', 'minut', 'misl', 'mistak', 'model', 'momma', 'mr', 'napeleon', 'near', 'nearest', 'need', 'network', 'new', 'news', 'notic', 'number', 'numer', 'offer', 'old', 'om', 'open', 'option', 'ori', 'ot', 'outbound', 'pass', 'pay', 'pda', 'peopl', 'perform', 'person', 'phone', 'piec', 'plan', 'pleas', 'point', 'polici', 'poor', 'possibl', 'probabl', 'problem', 'proper', 'provid', 'provis', 'purpos', 'rate', 'rater', 'realiz', 'realli', 'reason', 'receiv', 'recept', 'recption', 'reenter', 'refer', 'relat', 'rep', 'replac', 'respect', 'result', 'rid', 'right', 'ring', 'roam', 'roll', 'rubbish', 'rude', 'said', 'sale', 'say', 'screen', 'self', 'send', 'servic', 'shitti', 'shut', 'sign', 'signal', 'signific', 'simm', 'simpli', 'sinc', 'site', 'slow', 'sold', 'someon', 'sometim', 'soon', 'speak', 'speed', 'start', 'static', 'stole', 'store', 'stuff', 'stupid', 'substant', 'subtract', 'suck', 'suggest', 'supervisor', 'support', 'sure', 'surpris', 'suspect', 'suspend', 'switch', 'teach', 'technic', 'tell', 'terribl', 'test', 'text', 'think', 'thought', 'ticket', 'till', 'time', 'tire', 'today', 'toilet', 'told', 'tone', 'tower', 'transeff', 'transf', 'transfer', 'travel', 'tri', 'trust', 'turn', 'uncomfort', 'understand', 'unhappi', 'unlimit', 'unreli', 'unwil', 'upset', 'usag', 'use', 'useless', 'valu', 'veri', 'vm', 'wa', 'wait', 'want', 'wast', 'way', 'weak', 'web', 'websit', 'week', 'whi', 'wife', 'wish', 'wll', 'wold', 'work', 'wors', 'worst', 'wrong', 'xvyx', 'year', 'york']\n",
      "      0    1    2    3    4    5    6    ...  347  348  349  350  351  352  353\n",
      "0       0    0    0    0    0    0    0  ...    1    0    0    0    0    0    0\n",
      "1       0    0    0    0    1    0    0  ...    0    0    0    0    0    0    0\n",
      "2       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
      "3       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
      "4       0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "2065    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
      "2066    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
      "2067    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
      "2068    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
      "2069    0    0    0    0    0    0    0  ...    0    0    0    0    0    0    0\n",
      "\n",
      "[2070 rows x 354 columns]\n"
     ]
    }
   ],
   "source": [
    "#Do Bag-Of-Words model - Term - Document Matrix\n",
    "#Learn the vocabulary dictionary and return term-document matrix.\n",
    "#count_vect = CountVectorizer(stop_words=None)\n",
    "count_vect = CountVectorizer(stop_words='english',lowercase=False)\n",
    "TD_counts = count_vect.fit_transform(newTextData.CommentsTokenizedStemmed)\n",
    "print(TD_counts.shape)\n",
    "print(TD_counts.dtype)\n",
    "print(count_vect.get_feature_names())\n",
    "#print(TD_counts)\n",
    "DF_TD_Counts=pd.DataFrame(TD_counts.toarray())\n",
    "print(DF_TD_Counts)\n",
    "export_csv = DF_TD_Counts.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/TD_counts-TokenizedStemmed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pd8TZYnAxQbP",
    "outputId": "c0903964-3207-4eb2-9f42-f903e95c0a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 354)\n",
      "      0    1    2    3        4    5    ...  348  349  350  351  352  353\n",
      "0     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "1     0.0  0.0  0.0  0.0  0.27568  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "3     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "4     0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "...   ...  ...  ...  ...      ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "2065  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2066  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2067  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2068  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "2069  0.0  0.0  0.0  0.0  0.00000  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "\n",
      "[2070 rows x 354 columns]\n"
     ]
    }
   ],
   "source": [
    "#Compute TF-IDF Matrix\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(TD_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "DF_TF_IDF=pd.DataFrame(X_train_tfidf.toarray())\n",
    "print(DF_TF_IDF)\n",
    "export_csv= DF_TF_IDF.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/TFIDF_counts-TokenizedStemmed.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2owIUD6_eYO",
    "outputId": "86ad3685-87af-492c-98b5-ea589efa58d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 50)\n",
      "[  0  14  35  49  50  51  62  64  70  72  81  99 109 115 118 121 130 145\n",
      " 157 158 172 186 190 205 212 213 217 221 222 234 235 239 244 248 249 254\n",
      " 259 264 266 273 307 312 313 315 319 325 328 338 342 350]\n",
      "       0    1    2         3         4    5   ...   44   45        46   47   48   49\n",
      "0     0.0  0.0  0.0  0.000000  0.000000  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "1     0.0  0.0  0.0  0.000000  0.000000  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "2     0.0  0.0  0.0  0.000000  0.000000  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "3     0.0  0.0  0.0  0.000000  0.000000  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "4     0.0  0.0  0.0  0.000000  0.000000  0.0  ...  0.0  0.0  0.348322  0.0  0.0  0.0\n",
      "...   ...  ...  ...       ...       ...  ...  ...  ...  ...       ...  ...  ...  ...\n",
      "2065  0.0  0.0  0.0  0.000000  0.446161  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "2066  0.0  0.0  0.0  0.000000  0.000000  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "2067  0.0  0.0  0.0  0.000000  0.000000  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "2068  0.0  0.0  0.0  0.545354  0.000000  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "2069  0.0  0.0  0.0  0.000000  0.000000  0.0  ...  0.0  0.0  0.000000  0.0  0.0  0.0\n",
      "\n",
      "[2070 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "#Filter Method - Feature selection\n",
    "#Suppose, we select 50 features with top 50 Fisher scores\n",
    "selector = SelectKBest(k=50)\n",
    "#selector = SelectKBest(score_func=chi2, k=50)\n",
    "\n",
    "#new_DF_TF_IDF = SelectKBest(score_func=chi2, k=50).fit_transform(DF_TF_IDF,y_train)\n",
    "new_DF_TF_IDF = selector.fit_transform(DF_TF_IDF,y_train)\n",
    "print(new_DF_TF_IDF.shape)\n",
    "\n",
    "feature_names_out = selector.get_support(indices=True)\n",
    "print(feature_names_out)\n",
    "\n",
    "DF_TF_IDF_SelectedFeatures= pd.DataFrame(new_DF_TF_IDF)\n",
    "print(DF_TF_IDF_SelectedFeatures)\n",
    "\n",
    "export_csv= DF_TF_IDF_SelectedFeatures.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/TFIDF_counts-Selected Features.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ap5hcz78hPXn",
    "outputId": "6d0d73f5-bef8-461a-c1d7-1869dd4c39bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 17)\n",
      "(2070, 16)\n",
      "(2070, 66)\n",
      "        ID Sex Status  Children  Est_Income  ...   45        46   47   48   49\n",
      "0        1   F      S         1    38000.00  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "1        6   M      M         2    29616.00  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "2        8   M      M         0    19732.80  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "3       11   M      S         2       96.33  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "4       14   F      M         2    52004.80  ...  0.0  0.348322  0.0  0.0  0.0\n",
      "...    ...  ..    ...       ...         ...  ...  ...       ...  ...  ...  ...\n",
      "2065  3821   F      S         0    78851.30  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "2066  3822   F      S         1    17540.70  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "2067  3823   F      M         0    83891.90  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "2068  3824   F      M         2    28220.80  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "2069  3825   F      S         0    28589.10  ...  0.0  0.000000  0.0  0.0  0.0\n",
      "\n",
      "[2070 rows x 66 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combine text data with Customer Data\n",
    "print(CustInfoData.shape)\n",
    "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
    "print(X_train.shape)\n",
    "##combined=X_train.merge(DF_TF_IDF_SelectedFeatures, left_on='ID', right_on='ID')\n",
    "#combined=pd.merge(X_train, DF_TF_IDF_SelectedFeatures, how='left', on=['ID', 'ID'])\n",
    "#combined = pd.merge(X_train, DF_TF_IDF_SelectedFeatures, how='left',on = 'ID')\n",
    "#combined=pd.merge(X_train, DF_TF_IDF_SelectedFeatures, join ='inner', on='ID')\n",
    "\n",
    "combined=pd.concat([X_train, DF_TF_IDF_SelectedFeatures], axis=1)\n",
    "print(combined.shape)\n",
    "print(combined)\n",
    "export_csv= combined.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/Combined-Cust+TFIDF+SelectedFeatures.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_lMPsYXjhwN",
    "outputId": "a0fb86ea-49fc-497d-ada5-b625d5150aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex', 'Status', 'Car_Owner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype']\n",
      "(2070, 74)\n"
     ]
    }
   ],
   "source": [
    "#Do one Hot encoding for categorical features\n",
    "X_cat = [\"Sex\",\"Status\",\"Car_Owner\",\"Paymethod\",\"LocalBilltype\",\"LongDistanceBilltype\"]\n",
    "#X_cat = combined.select_dtypes(exclude=['int','float64'])\n",
    "print(X_cat)\n",
    "combined_one_hot = pd.get_dummies(combined,columns=X_cat)\n",
    "print(combined_one_hot.shape)\n",
    "export_csv= combined_one_hot.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/combined_one_hot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5QenAEbvjjln"
   },
   "outputs": [],
   "source": [
    "# Split the combined data\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(combined_one_hot, y_train, test_size = .20, \n",
    "                                                    stratify=y_train,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PzCe7ejkl77",
    "outputId": "4ee0fe66-95b2-42c4-97e0-aa528f8cca7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8888888888888888\n",
      "Confusion Matrix:\n",
      "[[132  29]\n",
      " [ 17 236]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Cancelled       0.89      0.82      0.85       161\n",
      "     Current       0.89      0.93      0.91       253\n",
      "\n",
      "    accuracy                           0.89       414\n",
      "   macro avg       0.89      0.88      0.88       414\n",
      "weighted avg       0.89      0.89      0.89       414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Construct a Random Forest Classifier on combined data (Filter method) and test accuracy\n",
    "clf=RandomForestClassifier()\n",
    "RF_Comb = clf.fit(X_train1,y_train1)\n",
    "rf_predictions = clf.predict(X_test1)\n",
    "print(\"Test Accuracy:\", metrics.accuracy_score(y_test1,rf_predictions))\n",
    "#print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_test1, y_test1)))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test1, rf_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test1, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQWZAk-LlugX",
    "outputId": "84e1b009-4cee-43cb-b74d-e0db323b3a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All Accuracy Scores ===\n",
      "[0.81939605 0.85114208 0.85423926 0.91153697 0.8703125  0.909375\n",
      " 0.8140625  0.8078125  0.8703125  0.9015625  0.85119048 0.89325397\n",
      " 0.93412698 0.81269841 0.91825397 0.81825397 0.89325397 0.97162698\n",
      " 0.85238095 0.93531746]\n",
      "\n",
      "\n",
      "=== Mean Accuracy Score ===\n",
      "Mean Accuracy Score - ON Text:  0.8745054503000388\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run cross-validation - COMBINED Data with feature selection using Filter method\n",
    "rf_Comb_cv_score = cross_val_score(RF_Comb, combined_one_hot, y_train, cv=20, scoring=\"balanced_accuracy\")\n",
    "print(\"=== All Accuracy Scores ===\")\n",
    "print(rf_Comb_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean Accuracy Score ===\")\n",
    "print(\"Mean Accuracy Score - ON Text: \",rf_Comb_cv_score.mean())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMRy9ZQDnqux",
    "outputId": "d9d233dc-808b-43d0-b60e-9e08d4209df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 1.81128213e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 6.67729452e-03 0.00000000e+00 7.92863179e-03\n",
      " 0.00000000e+00 0.00000000e+00 8.21329764e-02 0.00000000e+00\n",
      " 2.54549037e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 7.18165438e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.50452100e-02 6.01542168e-06\n",
      " 0.00000000e+00 0.00000000e+00 5.97890007e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.79273644e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.99491979e-03\n",
      " 0.00000000e+00 2.17324479e-02 1.18156083e-02 4.73499250e-05\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 9.40063931e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.16317724e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.45638332e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.83148535e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.71415887e-03 6.06142840e-06\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 6.79818477e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.82593737e-03 0.00000000e+00 1.53765487e-05\n",
      " 0.00000000e+00 0.00000000e+00 3.95282241e-02 0.00000000e+00\n",
      " 0.00000000e+00 2.68997851e-03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 8.49624339e-03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.10286524e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.66931713e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 5.26974831e-02 0.00000000e+00 0.00000000e+00\n",
      " 1.18387360e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.79485784e-05 1.27129478e-02\n",
      " 0.00000000e+00 4.59852369e-03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 6.71557544e-05 0.00000000e+00 0.00000000e+00\n",
      " 1.66864651e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 7.40299577e-03 3.24436620e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.54362579e-04 8.45473309e-03\n",
      " 0.00000000e+00 0.00000000e+00 7.17216139e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.14937657e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.60371367e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.68797902e-03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 6.55791887e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.44102019e-02 0.00000000e+00 1.46599989e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.56153779e-02 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 8.55052510e-02 0.00000000e+00 9.95820837e-02\n",
      " 1.48040145e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.42017158e-04 0.00000000e+00 9.68377409e-06 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.88323238e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 4.13646935e-02 0.00000000e+00 2.89928740e-03 1.48350893e-02\n",
      " 0.00000000e+00 0.00000000e+00 1.77350700e-02 0.00000000e+00\n",
      " 3.11324695e-03 0.00000000e+00 0.00000000e+00 4.16022321e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 7.59366110e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.59113356e-04\n",
      " 0.00000000e+00 0.00000000e+00 6.13225460e-03 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.59317165e-04\n",
      " 3.36502093e-03 2.23996660e-03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.21367173e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.72720690e-06 0.00000000e+00\n",
      " 0.00000000e+00 5.14808037e-02 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.08209292e-03 9.95659538e-04\n",
      " 0.00000000e+00 1.15076060e-03 6.40057500e-03 1.66754097e-02\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 2.39854783e-02 0.00000000e+00 0.00000000e+00 6.08692967e-04\n",
      " 0.00000000e+00 0.00000000e+00 3.42279153e-05 0.00000000e+00\n",
      " 0.00000000e+00 2.62710392e-03 0.00000000e+00 2.19265275e-03\n",
      " 0.00000000e+00 1.30867673e-02 1.27370521e-01 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.77183060e-03\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 2.25314461e-03 0.00000000e+00]\n",
      "[False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False]\n",
      "        0    1         2    3    4    5    6\n",
      "0     0.0  0.0  0.000000  0.0  0.0  0.0  0.0\n",
      "1     0.0  0.0  0.000000  0.0  0.0  0.0  0.0\n",
      "2     0.0  0.0  0.000000  0.0  0.0  0.0  0.0\n",
      "3     0.0  0.0  0.000000  0.0  0.0  0.0  0.0\n",
      "4     0.0  0.0  0.320855  0.0  0.0  0.0  0.0\n",
      "...   ...  ...       ...  ...  ...  ...  ...\n",
      "2065  0.0  0.0  0.324168  0.0  0.0  0.0  0.0\n",
      "2066  0.0  0.0  0.000000  0.0  0.0  0.0  0.0\n",
      "2067  0.0  0.0  0.000000  0.0  0.0  0.0  0.0\n",
      "2068  0.0  0.0  0.000000  0.0  0.0  0.0  0.0\n",
      "2069  0.0  0.0  0.000000  0.0  0.0  0.0  0.0\n",
      "\n",
      "[2070 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#Wrapper method -Do feature selection using a classification model\n",
    "#clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = GradientBoostingClassifier(n_estimators=50)\n",
    "#clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(DF_TF_IDF,y_train)\n",
    "print(clf.feature_importances_)\n",
    "#model = SelectFromModel(clf, prefit=True)\n",
    "model = SelectFromModel(clf, prefit=True, max_features=7, threshold=-np.inf)\n",
    "#model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(DF_TF_IDF)\n",
    "X_new_SelectedFeatures= pd.DataFrame(X_new)\n",
    "export_csv= X_new_SelectedFeatures.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/X_new_SelectedFeatures.csv')\n",
    "\n",
    "print(model.get_support())\n",
    "print(X_new_SelectedFeatures)\n",
    "#print(X_new_SelectedFeatures.shape)\n",
    "#print(X_new_SelectedFeatures.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R41JukfZyqGm",
    "outputId": "f9bdf962-4143-4a54-a8f5-9d8166c5ff3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 17)\n",
      "(2070, 16)\n",
      "(2070, 23)\n",
      "        ID Sex Status  Children  Est_Income  ...         2    3    4    5    6\n",
      "0        1   F      S         1    38000.00  ...  0.000000  0.0  0.0  0.0  0.0\n",
      "1        6   M      M         2    29616.00  ...  0.000000  0.0  0.0  0.0  0.0\n",
      "2        8   M      M         0    19732.80  ...  0.000000  0.0  0.0  0.0  0.0\n",
      "3       11   M      S         2       96.33  ...  0.000000  0.0  0.0  0.0  0.0\n",
      "4       14   F      M         2    52004.80  ...  0.320855  0.0  0.0  0.0  0.0\n",
      "...    ...  ..    ...       ...         ...  ...       ...  ...  ...  ...  ...\n",
      "2065  3821   F      S         0    78851.30  ...  0.324168  0.0  0.0  0.0  0.0\n",
      "2066  3822   F      S         1    17540.70  ...  0.000000  0.0  0.0  0.0  0.0\n",
      "2067  3823   F      M         0    83891.90  ...  0.000000  0.0  0.0  0.0  0.0\n",
      "2068  3824   F      M         2    28220.80  ...  0.000000  0.0  0.0  0.0  0.0\n",
      "2069  3825   F      S         0    28589.10  ...  0.000000  0.0  0.0  0.0  0.0\n",
      "\n",
      "[2070 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "#Combine new selected feature using Wrapper method with Customer Data\n",
    "print(CustInfoData.shape)\n",
    "X_train = CustInfoData.drop(columns=[\"TARGET\"]) #extracting training data without the target column\n",
    "print(X_train.shape)\n",
    "##combined=X_train.merge(DF_TF_IDF_SelectedFeatures, left_on='ID', right_on='ID')\n",
    "#combined=pd.merge(X_train, DF_TF_IDF_SelectedFeatures, how='left', on=['ID', 'ID'])\n",
    "#combined = pd.merge(X_train, DF_TF_IDF_SelectedFeatures, how='left',on = 'ID')\n",
    "#combined=pd.merge(X_train, DF_TF_IDF_SelectedFeatures, join ='inner', on='ID')\n",
    "\n",
    "combined_1=pd.concat([X_train, X_new_SelectedFeatures], axis=1)\n",
    "print(combined_1.shape)\n",
    "print(combined_1)\n",
    "#export_csv= combined.to_csv(r'/gdrive/My Drive/CIS508/Assignment_5/Combined-Cust+TFIDF+SelectedFeatures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K630XrUL0Zxn",
    "outputId": "c26d83be-5fbe-4686-bec9-318f0cb1c149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sex', 'Status', 'Car_Owner', 'Paymethod', 'LocalBilltype', 'LongDistanceBilltype']\n",
      "(2070, 31)\n"
     ]
    }
   ],
   "source": [
    "#Do one Hot encoding for categorical features\n",
    "X_cat = [\"Sex\",\"Status\",\"Car_Owner\",\"Paymethod\",\"LocalBilltype\",\"LongDistanceBilltype\"]\n",
    "#X_cat = combined.select_dtypes(exclude=['int','float64'])\n",
    "print(X_cat)\n",
    "combined_one_hot_1 = pd.get_dummies(combined_1,columns=X_cat)\n",
    "print(combined_one_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "7PuU2vFTzmsE"
   },
   "outputs": [],
   "source": [
    "# Split the combined data\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(combined_one_hot_1, y_train, test_size = .20, \n",
    "                                                    stratify=y_train,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geVCLka8xxjf",
    "outputId": "7f510c04-252d-4f22-818f-872e95789985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8864734299516909\n",
      "Confusion Matrix:\n",
      "[[134  27]\n",
      " [ 20 233]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Cancelled       0.87      0.83      0.85       161\n",
      "     Current       0.90      0.92      0.91       253\n",
      "\n",
      "    accuracy                           0.89       414\n",
      "   macro avg       0.88      0.88      0.88       414\n",
      "weighted avg       0.89      0.89      0.89       414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Construct a Random Forest Classifier - Combined Data(Wrapper method)\n",
    "clf=RandomForestClassifier()\n",
    "RF_Comb_1 = clf.fit(X_train1,y_train1)\n",
    "rf_predictions = clf.predict(X_test1)\n",
    "print(\"Test Accuracy:\", metrics.accuracy_score(y_test1,rf_predictions))\n",
    "#print(\"Accuracy score (training): {0:.6f}\".format(clf.score(X_test1, y_test1)))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test1, rf_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test1, rf_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkzeqH_L1WJT",
    "outputId": "c269492e-04c4-43f3-b795-a148b839b637"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All Accuracy Scores ===\n",
      "[0.85598142 0.87127371 0.8786295  0.92373209 0.8578125  0.9296875\n",
      " 0.84375    0.81875    0.8671875  0.88125    0.88869048 0.91825397\n",
      " 0.91369048 0.80019841 0.88075397 0.81031746 0.90238095 0.95912698\n",
      " 0.88988095 0.89444444]\n",
      "\n",
      "\n",
      "=== Mean Accuracy Score ===\n",
      "Mean Accuracy Score - ON Text:  0.8792896159988386\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run cross-validation - COMBINED Data(Wrapper Method)\n",
    "rf_Comb_cv_score = cross_val_score(RF_Comb_1, combined_one_hot_1, y_train, cv=20, scoring=\"balanced_accuracy\")\n",
    "print(\"=== All Accuracy Scores ===\")\n",
    "print(rf_Comb_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean Accuracy Score ===\")\n",
    "print(\"Mean Accuracy Score - ON Text: \",rf_Comb_cv_score.mean())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqwNFKiHLlbz",
    "outputId": "f6ff5e2e-e8c9-4238-ce75-e7599a2c649f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2070, 17)\n",
      "(2070, 9)\n",
      "   Children  Est_Income   Usage  ...  International   Local  Dropped\n",
      "0         1    38000.00  229.64  ...            0.0  206.08        0\n",
      "1         2    29616.00   75.29  ...            0.0   45.50        0\n",
      "2         0    19732.80   47.25  ...            0.0   22.44        0\n",
      "3         2       96.33   59.01  ...            0.0   32.88        1\n",
      "4         2    52004.80   28.14  ...            0.0   23.11        0\n",
      "\n",
      "[5 rows x 9 columns]\n",
      "(2070, 14)\n",
      "   Sex_F  ...  LongDistanceBilltype_Standard\n",
      "0      1  ...                              0\n",
      "1      0  ...                              1\n",
      "2      0  ...                              1\n",
      "3      0  ...                              1\n",
      "4      1  ...                              0\n",
      "\n",
      "[5 rows x 14 columns]\n",
      "(2070, 23)\n",
      "   Children  ...  LongDistanceBilltype_Standard\n",
      "0         1  ...                              0\n",
      "1         2  ...                              1\n",
      "2         0  ...                              1\n",
      "3         2  ...                              1\n",
      "4         2  ...                              0\n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Test Accuracy: 0.8768115942028986\n",
      "Confusion Matrix:\n",
      "[[130  31]\n",
      " [ 20 233]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Cancelled       0.87      0.81      0.84       161\n",
      "     Current       0.88      0.92      0.90       253\n",
      "\n",
      "    accuracy                           0.88       414\n",
      "   macro avg       0.87      0.86      0.87       414\n",
      "weighted avg       0.88      0.88      0.88       414\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Construct a Random Forest Classifier WITHOUT text data\n",
    "print(CustInfoData.shape)\n",
    "X_train1=combined_one_hot.iloc[:,1:10]\n",
    "X_train2=combined_one_hot.iloc[:,60:]\n",
    "print(X_train1.shape)\n",
    "print(X_train1.head())\n",
    "print(X_train2.shape)\n",
    "print(X_train2.head())\n",
    "combined1=pd.concat([X_train1, X_train2], axis=1)\n",
    "print(combined1.shape)\n",
    "print(combined1.head())\n",
    "\n",
    "# Split the data in train and test \n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(combined1, y_train, test_size = .20, \n",
    "                                                    stratify=y_train,random_state = 1)\n",
    "\n",
    "# Initiliaze Random Forest\n",
    "rf_NT=clf.fit(X_train1,y_train1)\n",
    "rf_predictions = rf_NT.predict(X_test1)\n",
    "print(\"Test Accuracy:\", metrics.accuracy_score(y_test1,rf_predictions))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test1, rf_predictions))\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test1, rf_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1MWgcUkWh0cI",
    "outputId": "b8a57738-e4a7-4ce8-edbf-c083f10b87f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== All Accuracy Scores ===\n",
      "[0.84436702 0.86759582 0.89508324 0.91153697 0.8953125  0.9015625\n",
      " 0.8515625  0.803125   0.875      0.90625    0.91031746 0.90119048\n",
      " 0.91825397 0.82063492 0.91031746 0.79325397 0.90238095 0.93869048\n",
      " 0.90238095 0.91825397]\n",
      "\n",
      "\n",
      "=== Mean Accuracy Score ===\n",
      "Mean Accuracy Score - WITHOUT Text:  0.8833535073073945\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run cross-validation - WITHOUT Text Data\n",
    "rf_NT_cv_score = cross_val_score(rf_NT,combined1,y_train, cv=20, scoring=\"balanced_accuracy\")\n",
    "print(\"=== All Accuracy Scores ===\")\n",
    "print(rf_NT_cv_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean Accuracy Score ===\")\n",
    "print(\"Mean Accuracy Score - WITHOUT Text: \",rf_NT_cv_score.mean())\n",
    "print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
